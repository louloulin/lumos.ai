//! ä¼ä¸šçº§ç›‘æ§ä»ªè¡¨æ¿æ¼”ç¤º
//! 
//! å±•ç¤ºLumos.aiçš„å®Œæ•´ç›‘æ§å’Œå¯è§‚æµ‹æ€§åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
//! - å®æ—¶æ€§èƒ½ç›‘æ§
//! - æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ
//! - æ€§èƒ½é¢„æµ‹åˆ†æ
//! - è‡ªåŠ¨åŒ–ä¼˜åŒ–å»ºè®®
//! - OpenTelemetryé›†æˆ

use std::sync::Arc;
use std::time::Duration;
use tokio::time::{sleep, interval};
use std::collections::HashMap;

use lumosai_core::telemetry::*;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸš€ å¯åŠ¨Lumos.aiä¼ä¸šçº§ç›‘æ§ä»ªè¡¨æ¿æ¼”ç¤º");
    println!("{}", "=".repeat(60));
    
    // åˆå§‹åŒ–ç›‘æ§ç³»ç»Ÿ
    let monitoring_system = MonitoringSystem::new().await?;
    
    // å¯åŠ¨ç›‘æ§ç³»ç»Ÿ
    monitoring_system.start().await?;
    
    // è¿è¡Œä»ªè¡¨æ¿æ¼”ç¤º
    run_dashboard_demo(&monitoring_system).await?;
    
    Ok(())
}

/// ç›‘æ§ç³»ç»Ÿé›†æˆ
struct MonitoringSystem {
    metrics_collector: Arc<DemoMetricsCollector>,
    performance_analyzer: Arc<DemoPerformanceAnalyzer>,
    alert_engine: SmartAlertEngine,
    performance_monitor: EnterprisePerformanceMonitor,
    otel_exporter: HttpOtlpExporter,
}

impl MonitoringSystem {
    async fn new() -> Result<Self, Box<dyn std::error::Error>> {
        // åˆ›å»ºæŒ‡æ ‡æ”¶é›†å™¨
        let metrics_collector = Arc::new(DemoMetricsCollector::new());
        
        // åˆ›å»ºæ€§èƒ½åˆ†æå™¨
        let performance_analyzer = Arc::new(DemoPerformanceAnalyzer::new());
        
        // åˆ›å»ºè‡ªåŠ¨åŒ–æ‰§è¡Œå™¨
        let automation_executor = Arc::new(DefaultAutomationExecutor::new(
            create_automation_config()
        ));
        
        // åˆ›å»ºæ™ºèƒ½å‘Šè­¦å¼•æ“
        let alert_engine = SmartAlertEngine::new(
            create_alert_engine_config(),
            metrics_collector.clone(),
            automation_executor,
        );
        
        // åˆ›å»ºä¼ä¸šçº§æ€§èƒ½ç›‘æ§å™¨
        let performance_monitor = EnterprisePerformanceMonitor::new(
            create_performance_monitor_config(),
            metrics_collector.clone(),
            performance_analyzer.clone(),
        );
        
        // åˆ›å»ºOpenTelemetryå¯¼å‡ºå™¨
        let otel_exporter = HttpOtlpExporter::new("http://localhost:4318".to_string())
            .with_timeout(Duration::from_secs(10));
        
        Ok(Self {
            metrics_collector,
            performance_analyzer,
            alert_engine,
            performance_monitor,
            otel_exporter,
        })
    }
    
    async fn start(&self) -> Result<(), Box<dyn std::error::Error>> {
        println!("ğŸ”§ é…ç½®ç›‘æ§ç³»ç»Ÿ...");
        
        // é…ç½®å‘Šè­¦è§„åˆ™
        self.setup_alert_rules().await?;
        
        // å¯åŠ¨å‘Šè­¦å¼•æ“
        if let Err(e) = self.alert_engine.start().await {
            return Err(format!("å¯åŠ¨å‘Šè­¦å¼•æ“å¤±è´¥: {}", e).into());
        }

        // å¯åŠ¨æ€§èƒ½ç›‘æ§
        if let Err(e) = self.performance_monitor.start().await {
            return Err(format!("å¯åŠ¨æ€§èƒ½ç›‘æ§å¤±è´¥: {}", e).into());
        }
        
        // å¯åŠ¨æ•°æ®ç”Ÿæˆå™¨
        self.start_data_generator().await;
        
        println!("âœ… ç›‘æ§ç³»ç»Ÿå¯åŠ¨å®Œæˆ");
        Ok(())
    }
    
    async fn setup_alert_rules(&self) -> Result<(), Box<dyn std::error::Error>> {
        // å“åº”æ—¶é—´å‘Šè­¦
        let response_time_rule = AlertRule {
            id: "response_time_critical".to_string(),
            name: "å“åº”æ—¶é—´ä¸¥é‡å‘Šè­¦".to_string(),
            description: "å½“å¹³å‡å“åº”æ—¶é—´è¶…è¿‡2ç§’æ—¶è§¦å‘ä¸¥é‡å‘Šè­¦".to_string(),
            condition: AlertCondition::ResponseTime {
                threshold_ms: 2000,
                window_minutes: 2,
                percentile: 95.0,
            },
            severity: AlertSeverity::Critical,
            enabled: true,
            channels: vec!["auto_scaling".to_string(), "notification".to_string()],
            labels: {
                let mut labels = HashMap::new();
                labels.insert("service".to_string(), "lumos-ai".to_string());
                labels.insert("environment".to_string(), "production".to_string());
                labels.insert("team".to_string(), "platform".to_string());
                labels
            },
            cooldown_duration: std::time::Duration::from_secs(300),
            created_at: std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_millis() as u64,
            updated_at: std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_millis() as u64,
        };
        
        // é”™è¯¯ç‡å‘Šè­¦
        let error_rate_rule = AlertRule {
            id: "error_rate_warning".to_string(),
            name: "é”™è¯¯ç‡è­¦å‘Š".to_string(),
            description: "å½“é”™è¯¯ç‡è¶…è¿‡3%æ—¶è§¦å‘è­¦å‘Š".to_string(),
            condition: AlertCondition::ErrorRate {
                threshold_percent: 3.0,
                window_minutes: 5,
                min_requests: 10,
            },
            severity: AlertSeverity::Warning,
            enabled: true,
            channels: vec!["notification".to_string()],
            labels: {
                let mut labels = HashMap::new();
                labels.insert("service".to_string(), "lumos-ai".to_string());
                labels.insert("component".to_string(), "agent-executor".to_string());
                labels
            },
            cooldown_duration: std::time::Duration::from_secs(300),
            created_at: std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_millis() as u64,
            updated_at: std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_millis() as u64,
        };
        
        // CPUä½¿ç”¨ç‡å‘Šè­¦
        let cpu_usage_rule = AlertRule {
            id: "cpu_usage_critical".to_string(),
            name: "CPUä½¿ç”¨ç‡ä¸¥é‡å‘Šè­¦".to_string(),
            description: "å½“CPUä½¿ç”¨ç‡è¶…è¿‡85%æ—¶è§¦å‘ä¸¥é‡å‘Šè­¦".to_string(),
            condition: AlertCondition::CpuUsage {
                threshold_percent: 85.0,
                window_minutes: 3,
            },
            severity: AlertSeverity::Critical,
            enabled: true,
            channels: vec!["auto_scaling".to_string(), "pagerduty".to_string()],
            labels: {
                let mut labels = HashMap::new();
                labels.insert("service".to_string(), "lumos-ai".to_string());
                labels.insert("resource".to_string(), "compute".to_string());
                labels
            },
            cooldown_duration: std::time::Duration::from_secs(300),
            created_at: std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_millis() as u64,
            updated_at: std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_millis() as u64,
        };
        
        if let Err(e) = self.alert_engine.add_rule(response_time_rule).await {
            return Err(format!("æ·»åŠ å“åº”æ—¶é—´å‘Šè­¦è§„åˆ™å¤±è´¥: {}", e).into());
        }
        if let Err(e) = self.alert_engine.add_rule(error_rate_rule).await {
            return Err(format!("æ·»åŠ é”™è¯¯ç‡å‘Šè­¦è§„åˆ™å¤±è´¥: {}", e).into());
        }
        if let Err(e) = self.alert_engine.add_rule(cpu_usage_rule).await {
            return Err(format!("æ·»åŠ CPUä½¿ç”¨ç‡å‘Šè­¦è§„åˆ™å¤±è´¥: {}", e).into());
        }
        
        println!("ğŸ“‹ é…ç½®äº†3ä¸ªå‘Šè­¦è§„åˆ™");
        Ok(())
    }
    
    async fn start_data_generator(&self) {
        let metrics_collector = self.metrics_collector.clone();
        tokio::spawn(async move {
            metrics_collector.start_generating_data().await;
        });
    }
}

/// è¿è¡Œä»ªè¡¨æ¿æ¼”ç¤º
async fn run_dashboard_demo(monitoring_system: &MonitoringSystem) -> Result<(), Box<dyn std::error::Error>> {
    println!("\nğŸ¯ å¼€å§‹ç›‘æ§ä»ªè¡¨æ¿æ¼”ç¤º");
    println!("{}", "=".repeat(60));
    
    let mut dashboard_interval = interval(Duration::from_secs(10));
    let mut demo_duration = 0;
    let max_demo_duration = 120; // 2åˆ†é’Ÿæ¼”ç¤º
    
    while demo_duration < max_demo_duration {
        dashboard_interval.tick().await;
        demo_duration += 10;
        
        println!("\nğŸ“Š ç›‘æ§ä»ªè¡¨æ¿ - ç¬¬{}ç§’", demo_duration);
        println!("{}", "-".repeat(50));
        
        // æ˜¾ç¤ºå®æ—¶æ€§èƒ½æŒ‡æ ‡
        display_performance_metrics(&monitoring_system.performance_monitor).await;
        
        // æ˜¾ç¤ºå‘Šè­¦çŠ¶æ€
        display_alert_status(&monitoring_system.alert_engine).await;
        
        // æ˜¾ç¤ºä¼˜åŒ–å»ºè®®
        display_optimization_suggestions(&monitoring_system.performance_monitor).await;
        
        // æ˜¾ç¤ºç³»ç»Ÿå¥åº·çŠ¶æ€
        display_system_health(&monitoring_system.performance_monitor).await;
        
        // æ¨¡æ‹ŸOpenTelemetryæ•°æ®å¯¼å‡º
        if demo_duration % 30 == 0 {
            simulate_otel_export(&monitoring_system.otel_exporter).await;
        }
        
        // åœ¨æ¼”ç¤ºä¸­æœŸæ¨¡æ‹Ÿæ€§èƒ½é—®é¢˜
        if demo_duration == 60 {
            println!("\nğŸ­ æ¨¡æ‹Ÿæ€§èƒ½é—®é¢˜åœºæ™¯...");
            monitoring_system.metrics_collector.simulate_performance_issue().await;
        }
    }
    
    // æ˜¾ç¤ºæœ€ç»ˆæŠ¥å‘Š
    display_final_report(monitoring_system).await;
    
    Ok(())
}

/// æ˜¾ç¤ºå®æ—¶æ€§èƒ½æŒ‡æ ‡
async fn display_performance_metrics(performance_monitor: &EnterprisePerformanceMonitor) {
    if let Some(metrics) = performance_monitor.get_current_performance_metrics().await {
        println!("ğŸ“ˆ å®æ—¶æ€§èƒ½æŒ‡æ ‡:");
        println!("   å“åº”æ—¶é—´: {:.2}ms (P95: {:.2}ms, P99: {:.2}ms)", 
            metrics.response_time.avg_ms, 
            metrics.response_time.p95_ms,
            metrics.response_time.p99_ms);
        println!("   ååé‡: {:.1} RPS ({} æ€»è¯·æ±‚)", 
            metrics.throughput.requests_per_second,
            metrics.throughput.total_requests);
        println!("   èµ„æºä½¿ç”¨: CPU {:.1}%, å†…å­˜ {:.1}%, ç£ç›˜ {:.1}%", 
            metrics.resource_usage.cpu_usage_percent,
            metrics.resource_usage.memory_usage_percent,
            metrics.resource_usage.disk_usage_percent);
        println!("   é”™è¯¯ç‡: {:.2}% ({} å¤±è´¥è¯·æ±‚)", 
            metrics.error_metrics.error_rate_percent,
            metrics.throughput.failed_requests);
        println!("   æ€§èƒ½è¶‹åŠ¿: {:?}", metrics.performance_trend);
    }
}

/// æ˜¾ç¤ºå‘Šè­¦çŠ¶æ€
async fn display_alert_status(alert_engine: &SmartAlertEngine) {
    let active_alerts = alert_engine.get_active_alerts().await;
    let stats = alert_engine.get_alert_statistics().await;
    
    println!("ğŸš¨ å‘Šè­¦çŠ¶æ€:");
    println!("   æ´»è·ƒå‘Šè­¦: {} ä¸ª", active_alerts.len());
    println!("   æ€»å‘Šè­¦æ•°: {}, å·²è§£å†³: {}", stats.total_alerts, stats.resolved_alerts);
    
    if !active_alerts.is_empty() {
        println!("   æœ€æ–°å‘Šè­¦:");
        for alert in active_alerts.iter().take(2) {
            println!("     - [{}] {}: {}", 
                format!("{:?}", alert.severity).to_uppercase(),
                alert.title, 
                alert.description);
        }
    }
    
    if stats.automation_executions > 0 {
        println!("   è‡ªåŠ¨åŒ–å“åº”: {} æ¬¡æ‰§è¡Œ, {:.1}% æˆåŠŸç‡", 
            stats.automation_executions,
            stats.automation_success_rate * 100.0);
    }
}

/// æ˜¾ç¤ºä¼˜åŒ–å»ºè®®
async fn display_optimization_suggestions(performance_monitor: &EnterprisePerformanceMonitor) {
    let suggestions = performance_monitor.get_optimization_suggestions().await;
    
    if !suggestions.is_empty() {
        println!("ğŸ’¡ ä¼˜åŒ–å»ºè®® ({} æ¡):", suggestions.len());
        for suggestion in suggestions.iter().take(2) {
            println!("   - {}: é¢„æœŸæ”¹å–„ {:.0}% (éš¾åº¦: {:?})", 
                suggestion.title,
                suggestion.expected_improvement,
                suggestion.implementation_difficulty);
        }
    }
}

/// æ˜¾ç¤ºç³»ç»Ÿå¥åº·çŠ¶æ€
async fn display_system_health(performance_monitor: &EnterprisePerformanceMonitor) {
    let summary = performance_monitor.get_performance_summary().await;
    let stats = performance_monitor.get_monitoring_statistics().await;
    
    let health_emoji = match summary.health_score {
        score if score >= 90.0 => "ğŸŸ¢",
        score if score >= 70.0 => "ğŸŸ¡",
        score if score >= 50.0 => "ğŸŸ ",
        _ => "ğŸ”´",
    };
    
    println!("ğŸ¥ ç³»ç»Ÿå¥åº·: {} {:.1}/100", health_emoji, summary.health_score);
    println!("   ç›‘æ§è¿è¡Œæ—¶é—´: {}ç§’, æ•°æ®ç‚¹: {}, é—®é¢˜æ£€æµ‹: {}", 
        stats.uptime_seconds,
        stats.total_data_points,
        stats.performance_issues_detected);
    
    if let Some(prediction) = summary.prediction {
        println!("ğŸ”® æ€§èƒ½é¢„æµ‹: å“åº”æ—¶é—´ {:.2}ms, ç½®ä¿¡åº¦ {:.0}%", 
            prediction.predicted_response_time_ms,
            prediction.confidence * 100.0);
    }
}

/// æ¨¡æ‹ŸOpenTelemetryæ•°æ®å¯¼å‡º
async fn simulate_otel_export(otel_exporter: &HttpOtlpExporter) {
    println!("ğŸ“¤ OpenTelemetryæ•°æ®å¯¼å‡º...");
    
    // åˆ›å»ºç¤ºä¾‹span
    let spans = vec![
        create_sample_span("agent_execution", 1500),
        create_sample_span("tool_invocation", 800),
    ];
    
    // åˆ›å»ºç¤ºä¾‹æŒ‡æ ‡
    let metrics = vec![
        create_sample_metric("response_time_ms", 1200.0),
        create_sample_metric("throughput_rps", 45.0),
    ];
    
    // å°è¯•å¯¼å‡ºï¼ˆåœ¨æ¼”ç¤ºä¸­ä¼šå¤±è´¥ï¼Œå› ä¸ºæ²¡æœ‰çœŸå®çš„OTLPç«¯ç‚¹ï¼‰
    match otel_exporter.export_spans(spans).await {
        Ok(_) => println!("   âœ… Spanså¯¼å‡ºæˆåŠŸ"),
        Err(_) => println!("   âš ï¸  Spanså¯¼å‡ºå¤±è´¥ï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼‰"),
    }
    
    match otel_exporter.export_metrics(metrics).await {
        Ok(_) => println!("   âœ… Metricså¯¼å‡ºæˆåŠŸ"),
        Err(_) => println!("   âš ï¸  Metricså¯¼å‡ºå¤±è´¥ï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼‰"),
    }
}

/// æ˜¾ç¤ºæœ€ç»ˆæŠ¥å‘Š
async fn display_final_report(monitoring_system: &MonitoringSystem) {
    println!("\nğŸ“‹ ç›‘æ§æ¼”ç¤ºæœ€ç»ˆæŠ¥å‘Š");
    println!("{}", "=".repeat(60));
    
    let alert_stats = monitoring_system.alert_engine.get_alert_statistics().await;
    let monitoring_stats = monitoring_system.performance_monitor.get_monitoring_statistics().await;
    let final_summary = monitoring_system.performance_monitor.get_performance_summary().await;
    
    println!("ğŸ“Š å‘Šè­¦ç³»ç»Ÿç»Ÿè®¡:");
    println!("   æ€»å‘Šè­¦æ•°: {}", alert_stats.total_alerts);
    println!("   æ´»è·ƒå‘Šè­¦: {}", alert_stats.active_alerts);
    println!("   å·²è§£å†³å‘Šè­¦: {}", alert_stats.resolved_alerts);
    println!("   å¹³å‡è§£å†³æ—¶é—´: {:.1} åˆ†é’Ÿ", alert_stats.avg_resolution_time_minutes);
    println!("   è‡ªåŠ¨åŒ–æ‰§è¡Œ: {} æ¬¡", alert_stats.automation_executions);
    
    println!("\nğŸ“ˆ æ€§èƒ½ç›‘æ§ç»Ÿè®¡:");
    println!("   ç›‘æ§è¿è¡Œæ—¶é—´: {} ç§’", monitoring_stats.uptime_seconds);
    println!("   æ”¶é›†æ•°æ®ç‚¹: {}", monitoring_stats.total_data_points);
    println!("   ç”Ÿæˆé¢„æµ‹: {}", monitoring_stats.predictions_generated);
    println!("   ä¼˜åŒ–å»ºè®®: {}", monitoring_stats.optimization_suggestions_provided);
    println!("   æ£€æµ‹é—®é¢˜: {}", monitoring_stats.performance_issues_detected);
    
    println!("\nğŸ¥ æœ€ç»ˆå¥åº·çŠ¶æ€: {:.1}/100", final_summary.health_score);
    
    println!("\nâœ… ä¼ä¸šçº§ç›‘æ§ä»ªè¡¨æ¿æ¼”ç¤ºå®Œæˆï¼");
    println!("ğŸ¯ å±•ç¤ºäº†Lumos.aiçš„å®Œæ•´ç›‘æ§å’Œå¯è§‚æµ‹æ€§èƒ½åŠ›");
}

/// åˆ›å»ºå‘Šè­¦å¼•æ“é…ç½®
fn create_alert_engine_config() -> AlertEngineConfig {
    let mut automation_actions = HashMap::new();

    // è‡ªåŠ¨æ‰©å®¹æ“ä½œ
    automation_actions.insert("auto_scaling".to_string(), AutomationAction {
        name: "è‡ªåŠ¨æ‰©å®¹".to_string(),
        action_type: AutomationActionType::ScaleUp,
        parameters: {
            let mut params = HashMap::new();
            params.insert("min_instances".to_string(), serde_json::json!(2));
            params.insert("max_instances".to_string(), serde_json::json!(10));
            params.insert("scale_factor".to_string(), serde_json::json!(1.5));
            params
        },
        trigger_conditions: vec!["cpu_high".to_string(), "response_time_high".to_string()],
        enabled: true,
    });

    // é€šçŸ¥æ“ä½œ
    automation_actions.insert("notification".to_string(), AutomationAction {
        name: "å‘é€é€šçŸ¥".to_string(),
        action_type: AutomationActionType::SendNotification,
        parameters: {
            let mut params = HashMap::new();
            params.insert("channels".to_string(), serde_json::json!(["email", "slack"]));
            params.insert("urgency".to_string(), serde_json::json!("high"));
            params
        },
        trigger_conditions: vec!["any_alert".to_string()],
        enabled: true,
    });

    AlertEngineConfig {
        check_interval_seconds: 5, // æ›´é¢‘ç¹çš„æ£€æŸ¥ç”¨äºæ¼”ç¤º
        max_concurrent_alerts: 50,
        deduplication_window_seconds: 60,
        auto_recovery_check_seconds: 10,
        escalation_config: EscalationConfig {
            enabled: true,
            escalation_time_minutes: 2, // å¿«é€Ÿå‡çº§ç”¨äºæ¼”ç¤º
            severity_escalation: {
                let mut map = HashMap::new();
                map.insert(AlertSeverity::Warning, AlertSeverity::Critical);
                map.insert(AlertSeverity::Critical, AlertSeverity::Critical);
                map
            },
        },
        automation_config: AutomationConfig {
            enabled: true,
            actions: automation_actions,
            action_timeout_seconds: 30,
        },
    }
}

/// åˆ›å»ºæ€§èƒ½ç›‘æ§é…ç½®
fn create_performance_monitor_config() -> PerformanceMonitorConfig {
    PerformanceMonitorConfig {
        monitoring_interval_seconds: 3, // æ›´é¢‘ç¹çš„ç›‘æ§ç”¨äºæ¼”ç¤º
        data_retention_hours: 1, // çŸ­æœŸä¿ç•™ç”¨äºæ¼”ç¤º
        thresholds: PerformanceThresholds {
            response_time_ms: 1000.0,
            cpu_usage_percent: 75.0,
            memory_usage_percent: 80.0,
            error_rate_percent: 2.0,
            throughput_rps: 50.0,
        },
        prediction_config: PredictionConfig {
            enabled: true,
            prediction_window_hours: 1,
            history_window_hours: 1,
            accuracy_threshold: 0.7,
        },
        auto_optimization_config: AutoOptimizationConfig {
            enabled: true,
            strategies: vec![
                OptimizationStrategy::AutoScaling,
                OptimizationStrategy::CacheOptimization,
                OptimizationStrategy::ConnectionPoolTuning,
            ],
            execution_interval_minutes: 2, // å¿«é€Ÿä¼˜åŒ–å»ºè®®ç”¨äºæ¼”ç¤º
        },
    }
}

/// åˆ›å»ºè‡ªåŠ¨åŒ–é…ç½®
fn create_automation_config() -> AutomationConfig {
    let mut actions = HashMap::new();

    actions.insert("restart_service".to_string(), AutomationAction {
        name: "é‡å¯æœåŠ¡".to_string(),
        action_type: AutomationActionType::RestartService,
        parameters: HashMap::new(),
        trigger_conditions: vec!["service_unhealthy".to_string()],
        enabled: true,
    });

    AutomationConfig {
        enabled: true,
        actions,
        action_timeout_seconds: 60,
    }
}

/// åˆ›å»ºç¤ºä¾‹span
fn create_sample_span(operation_name: &str, duration_ms: u64) -> OtelSpan {
    let now = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_millis() as u64;

    let mut attributes = HashMap::new();
    attributes.insert("service.name".to_string(), AttributeValue::String("lumos-ai".to_string()));
    attributes.insert("operation.name".to_string(), AttributeValue::String(operation_name.to_string()));
    attributes.insert("duration.ms".to_string(), AttributeValue::Int(duration_ms as i64));

    OtelSpan {
        trace_id: format!("trace-{}", uuid::Uuid::new_v4()),
        span_id: format!("span-{}", uuid::Uuid::new_v4()),
        parent_span_id: None,
        name: operation_name.to_string(),
        kind: SpanKind::Internal,
        start_time_ns: (now - duration_ms) * 1_000_000,
        end_time_ns: now * 1_000_000,
        status: SpanStatus::Ok,
        attributes,
        events: vec![],
    }
}

/// åˆ›å»ºç¤ºä¾‹æŒ‡æ ‡
fn create_sample_metric(metric_name: &str, value: f64) -> OtelMetric {
    let now = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_millis() as u64;

    let mut attributes = HashMap::new();
    attributes.insert("service.name".to_string(), AttributeValue::String("lumos-ai".to_string()));

    OtelMetric {
        name: metric_name.to_string(),
        description: format!("Sample metric: {}", metric_name),
        unit: "".to_string(),
        data_points: vec![
            DataPoint {
                timestamp_ns: now * 1_000_000,
                value: DataPointValue::Double(value),
                attributes,
            }
        ],
    }
}

/// æ¼”ç¤ºæŒ‡æ ‡æ”¶é›†å™¨
struct DemoMetricsCollector {
    performance_issue_mode: Arc<tokio::sync::RwLock<bool>>,
    execution_count: Arc<tokio::sync::RwLock<u64>>,
}

impl DemoMetricsCollector {
    fn new() -> Self {
        Self {
            performance_issue_mode: Arc::new(tokio::sync::RwLock::new(false)),
            execution_count: Arc::new(tokio::sync::RwLock::new(0)),
        }
    }

    async fn start_generating_data(&self) {
        let mut interval = tokio::time::interval(Duration::from_secs(1));
        let execution_count = self.execution_count.clone();

        loop {
            interval.tick().await;
            let mut count = execution_count.write().await;
            *count += 1;
        }
    }

    async fn simulate_performance_issue(&self) {
        let mut issue_mode = self.performance_issue_mode.write().await;
        *issue_mode = true;
        println!("ğŸ­ å¯ç”¨æ€§èƒ½é—®é¢˜æ¨¡æ‹Ÿæ¨¡å¼");

        // 30ç§’åæ¢å¤æ­£å¸¸
        let issue_mode_clone = self.performance_issue_mode.clone();
        tokio::spawn(async move {
            sleep(Duration::from_secs(30)).await;
            let mut issue_mode = issue_mode_clone.write().await;
            *issue_mode = false;
            println!("âœ… æ€§èƒ½é—®é¢˜æ¨¡æ‹Ÿæ¨¡å¼å·²æ¢å¤");
        });
    }
}

#[async_trait::async_trait]
impl MetricsCollector for DemoMetricsCollector {
    async fn record_agent_execution(&self, _metrics: AgentMetrics) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        Ok(())
    }

    async fn record_tool_execution(&self, _metrics: ToolMetrics) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        Ok(())
    }

    async fn record_memory_operation(&self, _metrics: MemoryMetrics) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        Ok(())
    }

    async fn get_agent_performance(&self, _agent_id: &str) -> Result<AgentPerformance, Box<dyn std::error::Error + Send + Sync>> {
        let now = std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_millis() as u64;
        Ok(AgentPerformance {
            agent_name: _agent_id.to_string(),
            executions_last_24h: 100,
            success_rate_24h: 95.0,
            avg_response_time_24h: 800.0,
            error_rate_trend: vec![(now - 3600000, 0.05), (now, 0.03)],
            performance_trend: vec![(now - 3600000, 850.0), (now, 800.0)],
            top_tools: vec![("web_search".to_string(), 50), ("file_read".to_string(), 30)],
            resource_usage: ResourceUsage {
                avg_memory_mb: 256.0,
                peak_memory_mb: 512.0,
                cpu_usage_percent: 15.0,
            },
        })
    }

    async fn get_metrics_summary(
        &self,
        _agent_id: Option<&str>,
        _start_time: Option<u64>,
        _end_time: Option<u64>,
    ) -> Result<MetricsSummary, Box<dyn std::error::Error + Send + Sync>> {
        let execution_count = *self.execution_count.read().await;
        let is_issue_mode = *self.performance_issue_mode.read().await;

        // æ ¹æ®æ˜¯å¦å¤„äºé—®é¢˜æ¨¡å¼è°ƒæ•´æŒ‡æ ‡
        let (avg_time, error_rate) = if is_issue_mode {
            (2500.0, 8.0) // é—®é¢˜æ¨¡å¼ï¼šé«˜å»¶è¿Ÿå’Œé”™è¯¯ç‡
        } else {
            (800.0, 1.5) // æ­£å¸¸æ¨¡å¼
        };

        let total_executions = execution_count * 10;
        let failed_executions = (total_executions as f64 * error_rate / 100.0) as u64;

        Ok(MetricsSummary {
            total_executions,
            successful_executions: total_executions - failed_executions,
            failed_executions,
            avg_execution_time_ms: avg_time,
            total_tokens_used: total_executions * 50,
            avg_tokens_per_execution: 50.0,
            min_execution_time_ms: (avg_time * 0.5) as u64,
            max_execution_time_ms: (avg_time * 2.0) as u64,
            tool_call_stats: std::collections::HashMap::new(),
            time_range: TimeRange {
                start: std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_millis() as u64 - 3600000,
                end: std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_millis() as u64,
            },
        })
    }
}

/// æ¼”ç¤ºæ€§èƒ½åˆ†æå™¨
struct DemoPerformanceAnalyzer;

impl DemoPerformanceAnalyzer {
    fn new() -> Self {
        Self
    }
}

#[async_trait::async_trait]
impl PerformanceAnalyzer for DemoPerformanceAnalyzer {
    async fn analyze(
        &self,
        metrics: &[AgentMetrics],
        time_range: TimeRange,
    ) -> Result<PerformanceAnalysis, Box<dyn std::error::Error + Send + Sync>> {
        let score = if metrics.is_empty() {
            50.0
        } else {
            let avg_time = metrics.iter().map(|m| m.execution_time_ms as f64).sum::<f64>() / metrics.len() as f64;
            if avg_time > 2000.0 { 45.0 } else { 85.0 }
        };

        let now = std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH).unwrap().as_millis() as u64;

        Ok(PerformanceAnalysis {
            overall_score: score,
            bottlenecks: vec![],
            anomalies: vec![],
            recommendations: vec![],
            trend: PerformanceTrend::Stable { variance: 0.1 },
            predictions: vec![],
            time_range,
            timestamp: now,
        })
    }

    async fn detect_anomalies(&self, _metrics: &[AgentMetrics]) -> Result<Vec<lumosai_core::telemetry::analyzer::PerformanceAnomaly>, Box<dyn std::error::Error + Send + Sync>> {
        Ok(vec![])
    }

    async fn identify_bottlenecks(&self, _metrics: &[AgentMetrics]) -> Result<Vec<lumosai_core::telemetry::analyzer::PerformanceBottleneck>, Box<dyn std::error::Error + Send + Sync>> {
        Ok(vec![])
    }

    async fn generate_recommendations(&self, _analysis: &PerformanceAnalysis) -> Result<Vec<OptimizationRecommendation>, Box<dyn std::error::Error + Send + Sync>> {
        Ok(vec![])
    }

    async fn predict_trends(&self, _metrics: &[AgentMetrics]) -> Result<Vec<PerformancePrediction>, Box<dyn std::error::Error + Send + Sync>> {
        Ok(vec![])
    }
}
