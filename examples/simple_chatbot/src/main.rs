//! ç®€å•èŠå¤©æœºå™¨äººç¤ºä¾‹
//! 
//! è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ LumosAI åˆ›å»ºä¸€ä¸ªç®€å•çš„å‘½ä»¤è¡ŒèŠå¤©æœºå™¨äººã€‚
//! 
//! è¿è¡Œæ–¹å¼:
//! ```bash
//! cargo run --example simple_chatbot
//! ```

use anyhow::Result;
use clap::Parser;
use lumosai::prelude::*;
use std::io::{self, Write};

#[derive(Parser)]
#[command(name = "simple_chatbot")]
#[command(about = "ä¸€ä¸ªä½¿ç”¨ LumosAI æž„å»ºçš„ç®€å•èŠå¤©æœºå™¨äºº")]
struct Args {
    /// LLM æ¨¡åž‹åç§°
    #[arg(short, long, default_value = "gpt-3.5-turbo")]
    model: String,
    
    /// ç³»ç»Ÿæç¤º
    #[arg(short, long, default_value = "ä½ æ˜¯ä¸€ä¸ªå‹å–„çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›žç­”é—®é¢˜ã€‚")]
    system_prompt: String,
    
    /// æ¸©åº¦å‚æ•° (0.0-2.0)
    #[arg(short, long, default_value = "0.7")]
    temperature: f32,
}

#[tokio::main]
async fn main() -> Result<()> {
    let args = Args::parse();
    
    println!("ðŸ¤– LumosAI èŠå¤©æœºå™¨äºº");
    println!("æ¨¡åž‹: {}", args.model);
    println!("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º\n");
    
    // åˆ›å»º Agent
    let agent = create_agent(&args).await?;
    
    // å¼€å§‹èŠå¤©å¾ªçŽ¯
    chat_loop(agent).await?;
    
    Ok(())
}

async fn create_agent(args: &Args) -> Result<impl Agent> {
    // è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„ API åˆ›å»º Agent
    // åœ¨å®žé™…å®žçŽ°ä¸­ï¼Œè¿™ä¼šè°ƒç”¨æˆ‘ä»¬çš„ç»Ÿä¸€ API
    let agent = lumosai::agent::builder()
        .model(&args.model)
        .system_prompt(&args.system_prompt)
        .temperature(args.temperature)
        .build()
        .await?;
    
    Ok(agent)
}

async fn chat_loop(agent: impl Agent) -> Result<()> {
    let mut conversation_history = Vec::new();
    
    loop {
        // èŽ·å–ç”¨æˆ·è¾“å…¥
        print!("ðŸ‘¤ ä½ : ");
        io::stdout().flush()?;
        
        let mut input = String::new();
        io::stdin().read_line(&mut input)?;
        let input = input.trim();
        
        // æ£€æŸ¥é€€å‡ºå‘½ä»¤
        if input.is_empty() {
            continue;
        }
        
        if input == "quit" || input == "exit" {
            println!("ðŸ‘‹ å†è§ï¼");
            break;
        }
        
        // ç‰¹æ®Šå‘½ä»¤å¤„ç†
        if input.starts_with('/') {
            handle_command(input, &agent).await?;
            continue;
        }
        
        // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°åŽ†å²
        conversation_history.push(Message::user(input));
        
        // èŽ·å– AI å›žå¤
        print!("ðŸ¤– AI: ");
        io::stdout().flush()?;
        
        match agent.chat_with_history(input, &conversation_history).await {
            Ok(response) => {
                println!("{}", response);
                conversation_history.push(Message::assistant(&response));
            }
            Err(e) => {
                eprintln!("âŒ é”™è¯¯: {}", e);
            }
        }
        
        println!(); // ç©ºè¡Œåˆ†éš”
    }
    
    Ok(())
}

async fn handle_command(command: &str, agent: &impl Agent) -> Result<()> {
    match command {
        "/help" => {
            println!("ðŸ“– å¯ç”¨å‘½ä»¤:");
            println!("  /help     - æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯");
            println!("  /stats    - æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯");
            println!("  /clear    - æ¸…é™¤å¯¹è¯åŽ†å²");
            println!("  /info     - æ˜¾ç¤º Agent ä¿¡æ¯");
        }
        "/stats" => {
            // æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
            if let Ok(stats) = agent.get_stats().await {
                println!("ðŸ“Š ç»Ÿè®¡ä¿¡æ¯:");
                println!("  æ€»å¯¹è¯æ•°: {}", stats.total_conversations);
                println!("  å¹³å‡å“åº”æ—¶é—´: {:?}", stats.average_response_time);
                println!("  æˆåŠŸçŽ‡: {:.2}%", stats.success_rate * 100.0);
            }
        }
        "/clear" => {
            println!("ðŸ§¹ å¯¹è¯åŽ†å²å·²æ¸…é™¤");
        }
        "/info" => {
            println!("â„¹ï¸ Agent ä¿¡æ¯:");
            println!("  æ¨¡åž‹: {}", agent.model_name());
            println!("  æ¸©åº¦: {}", agent.temperature());
            println!("  ç³»ç»Ÿæç¤º: {}", agent.system_prompt());
        }
        _ => {
            println!("â“ æœªçŸ¥å‘½ä»¤: {}ï¼Œè¾“å…¥ /help æŸ¥çœ‹å¸®åŠ©", command);
        }
    }
    
    Ok(())
}

// æ¨¡æ‹Ÿçš„ Agent trait å’Œç›¸å…³ç±»åž‹
// åœ¨å®žé™…å®žçŽ°ä¸­ï¼Œè¿™äº›ä¼šåœ¨ lumosai crate ä¸­å®šä¹‰
trait Agent {
    async fn chat_with_history(&self, message: &str, history: &[Message]) -> Result<String>;
    async fn get_stats(&self) -> Result<AgentStats>;
    fn model_name(&self) -> &str;
    fn temperature(&self) -> f32;
    fn system_prompt(&self) -> &str;
}

#[derive(Debug)]
struct Message {
    role: String,
    content: String,
}

impl Message {
    fn user(content: &str) -> Self {
        Self {
            role: "user".to_string(),
            content: content.to_string(),
        }
    }
    
    fn assistant(content: &str) -> Self {
        Self {
            role: "assistant".to_string(),
            content: content.to_string(),
        }
    }
}

#[derive(Debug)]
struct AgentStats {
    total_conversations: u64,
    average_response_time: std::time::Duration,
    success_rate: f64,
}

// æ¨¡æ‹Ÿçš„ Agent å®žçŽ°
struct SimpleAgent {
    model: String,
    system_prompt: String,
    temperature: f32,
    stats: AgentStats,
}

impl Agent for SimpleAgent {
    async fn chat_with_history(&self, message: &str, _history: &[Message]) -> Result<String> {
        // æ¨¡æ‹Ÿ AI å›žå¤
        tokio::time::sleep(std::time::Duration::from_millis(500)).await;
        
        let responses = vec![
            "è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰è¶£çš„é—®é¢˜ï¼",
            "è®©æˆ‘æƒ³æƒ³...",
            "æ ¹æ®æˆ‘çš„ç†è§£ï¼Œ",
            "è¿™ä¸ªè¯é¢˜å¾ˆå¤æ‚ï¼Œ",
            "æˆ‘è®¤ä¸º...",
        ];
        
        let response = responses[message.len() % responses.len()];
        Ok(format!("{} {}", response, message))
    }
    
    async fn get_stats(&self) -> Result<AgentStats> {
        Ok(AgentStats {
            total_conversations: 42,
            average_response_time: std::time::Duration::from_millis(750),
            success_rate: 0.95,
        })
    }
    
    fn model_name(&self) -> &str {
        &self.model
    }
    
    fn temperature(&self) -> f32 {
        self.temperature
    }
    
    fn system_prompt(&self) -> &str {
        &self.system_prompt
    }
}

// æ¨¡æ‹Ÿçš„ lumosai API
mod lumosai {
    pub mod prelude {
        pub use super::agent::*;
    }
    
    pub mod agent {
        use super::super::*;
        
        pub fn builder() -> AgentBuilder {
            AgentBuilder::default()
        }
        
        #[derive(Default)]
        pub struct AgentBuilder {
            model: Option<String>,
            system_prompt: Option<String>,
            temperature: Option<f32>,
        }
        
        impl AgentBuilder {
            pub fn model(mut self, model: &str) -> Self {
                self.model = Some(model.to_string());
                self
            }
            
            pub fn system_prompt(mut self, prompt: &str) -> Self {
                self.system_prompt = Some(prompt.to_string());
                self
            }
            
            pub fn temperature(mut self, temp: f32) -> Self {
                self.temperature = Some(temp);
                self
            }
            
            pub async fn build(self) -> Result<SimpleAgent> {
                Ok(SimpleAgent {
                    model: self.model.unwrap_or_else(|| "gpt-3.5-turbo".to_string()),
                    system_prompt: self.system_prompt.unwrap_or_else(|| "You are a helpful assistant".to_string()),
                    temperature: self.temperature.unwrap_or(0.7),
                    stats: AgentStats {
                        total_conversations: 0,
                        average_response_time: std::time::Duration::from_millis(0),
                        success_rate: 1.0,
                    },
                })
            }
        }
    }
}
