//! ä¼ä¸šçº§æ€§èƒ½ç›‘æ§ç³»ç»Ÿ - å®æ—¶æ€§èƒ½åˆ†æå’Œä¼˜åŒ–å»ºè®®
//! 
//! æä¾›æ·±åº¦æ€§èƒ½åˆ†æã€ç“¶é¢ˆè¯†åˆ«ã€é¢„æµ‹æ€§ç›‘æ§å’Œè‡ªåŠ¨åŒ–ä¼˜åŒ–å»ºè®®

use serde::{Deserialize, Serialize};
use std::collections::{HashMap, VecDeque};
use std::sync::Arc;
use tokio::sync::RwLock;
use std::time::{SystemTime, UNIX_EPOCH, Duration};
use async_trait::async_trait;

use super::metrics::{MetricsCollector, AgentMetrics, ToolMetrics, MetricsSummary};
use super::analyzer::{PerformanceAnalyzer, PerformanceAnalysis, BottleneckType, OptimizationRecommendation};

/// æ€§èƒ½ç›‘æ§é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceMonitorConfig {
    /// ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰
    pub monitoring_interval_seconds: u64,
    /// æ€§èƒ½æ•°æ®ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰
    pub data_retention_hours: u64,
    /// æ€§èƒ½é˜ˆå€¼é…ç½®
    pub thresholds: PerformanceThresholds,
    /// é¢„æµ‹é…ç½®
    pub prediction_config: PredictionConfig,
    /// è‡ªåŠ¨ä¼˜åŒ–é…ç½®
    pub auto_optimization_config: AutoOptimizationConfig,
}

/// æ€§èƒ½é˜ˆå€¼é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceThresholds {
    /// å“åº”æ—¶é—´é˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰
    pub response_time_ms: f64,
    /// CPUä½¿ç”¨ç‡é˜ˆå€¼ï¼ˆç™¾åˆ†æ¯”ï¼‰
    pub cpu_usage_percent: f64,
    /// å†…å­˜ä½¿ç”¨ç‡é˜ˆå€¼ï¼ˆç™¾åˆ†æ¯”ï¼‰
    pub memory_usage_percent: f64,
    /// é”™è¯¯ç‡é˜ˆå€¼ï¼ˆç™¾åˆ†æ¯”ï¼‰
    pub error_rate_percent: f64,
    /// ååé‡é˜ˆå€¼ï¼ˆè¯·æ±‚/ç§’ï¼‰
    pub throughput_rps: f64,
}

/// é¢„æµ‹é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PredictionConfig {
    /// æ˜¯å¦å¯ç”¨é¢„æµ‹
    pub enabled: bool,
    /// é¢„æµ‹æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
    pub prediction_window_hours: u64,
    /// å†å²æ•°æ®çª—å£ï¼ˆå°æ—¶ï¼‰
    pub history_window_hours: u64,
    /// é¢„æµ‹ç²¾åº¦è¦æ±‚
    pub accuracy_threshold: f64,
}

/// è‡ªåŠ¨ä¼˜åŒ–é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AutoOptimizationConfig {
    /// æ˜¯å¦å¯ç”¨è‡ªåŠ¨ä¼˜åŒ–
    pub enabled: bool,
    /// ä¼˜åŒ–ç­–ç•¥
    pub strategies: Vec<OptimizationStrategy>,
    /// ä¼˜åŒ–æ‰§è¡Œé—´éš”ï¼ˆåˆ†é’Ÿï¼‰
    pub execution_interval_minutes: u64,
}

/// ä¼˜åŒ–ç­–ç•¥
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum OptimizationStrategy {
    /// è‡ªåŠ¨æ‰©å®¹
    AutoScaling,
    /// ç¼“å­˜ä¼˜åŒ–
    CacheOptimization,
    /// è¿æ¥æ± è°ƒä¼˜
    ConnectionPoolTuning,
    /// åƒåœ¾å›æ”¶ä¼˜åŒ–
    GarbageCollectionTuning,
    /// è´Ÿè½½å‡è¡¡è°ƒæ•´
    LoadBalancingAdjustment,
}

/// å®æ—¶æ€§èƒ½æŒ‡æ ‡
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RealTimePerformanceMetrics {
    /// æ—¶é—´æˆ³
    pub timestamp: u64,
    /// å“åº”æ—¶é—´ç»Ÿè®¡
    pub response_time: ResponseTimeMetrics,
    /// ååé‡ç»Ÿè®¡
    pub throughput: ThroughputMetrics,
    /// èµ„æºä½¿ç”¨ç»Ÿè®¡
    pub resource_usage: ResourceUsageMetrics,
    /// é”™è¯¯ç»Ÿè®¡
    pub error_metrics: ErrorMetrics,
    /// æ€§èƒ½è¶‹åŠ¿
    pub performance_trend: PerformanceTrend,
}

/// å“åº”æ—¶é—´æŒ‡æ ‡
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ResponseTimeMetrics {
    /// å¹³å‡å“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub avg_ms: f64,
    /// ä¸­ä½æ•°å“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub median_ms: f64,
    /// 95åˆ†ä½æ•°å“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub p95_ms: f64,
    /// 99åˆ†ä½æ•°å“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub p99_ms: f64,
    /// æœ€å¤§å“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub max_ms: f64,
    /// æœ€å°å“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub min_ms: f64,
}

/// ååé‡æŒ‡æ ‡
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ThroughputMetrics {
    /// æ¯ç§’è¯·æ±‚æ•°
    pub requests_per_second: f64,
    /// æ¯åˆ†é’Ÿè¯·æ±‚æ•°
    pub requests_per_minute: f64,
    /// æ€»è¯·æ±‚æ•°
    pub total_requests: u64,
    /// æˆåŠŸè¯·æ±‚æ•°
    pub successful_requests: u64,
    /// å¤±è´¥è¯·æ±‚æ•°
    pub failed_requests: u64,
}

/// èµ„æºä½¿ç”¨æŒ‡æ ‡
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ResourceUsageMetrics {
    /// CPUä½¿ç”¨ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
    pub cpu_usage_percent: f64,
    /// å†…å­˜ä½¿ç”¨ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
    pub memory_usage_percent: f64,
    /// ç£ç›˜ä½¿ç”¨ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
    pub disk_usage_percent: f64,
    /// ç½‘ç»œå¸¦å®½ä½¿ç”¨ï¼ˆMB/sï¼‰
    pub network_bandwidth_mbps: f64,
    /// æ´»è·ƒè¿æ¥æ•°
    pub active_connections: u64,
}

/// é”™è¯¯æŒ‡æ ‡
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ErrorMetrics {
    /// é”™è¯¯ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
    pub error_rate_percent: f64,
    /// æŒ‰ç±»å‹åˆ†ç»„çš„é”™è¯¯æ•°é‡
    pub errors_by_type: HashMap<String, u64>,
    /// æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„çš„é”™è¯¯æ•°é‡
    pub errors_by_severity: HashMap<String, u64>,
    /// æœ€è¿‘é”™è¯¯åˆ—è¡¨
    pub recent_errors: Vec<ErrorEvent>,
}

/// é”™è¯¯äº‹ä»¶
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ErrorEvent {
    /// é”™è¯¯ID
    pub id: String,
    /// é”™è¯¯ç±»å‹
    pub error_type: String,
    /// é”™è¯¯æ¶ˆæ¯
    pub message: String,
    /// å‘ç”Ÿæ—¶é—´
    pub timestamp: u64,
    /// ä¸¥é‡ç¨‹åº¦
    pub severity: String,
    /// ç›¸å…³ä¸Šä¸‹æ–‡
    pub context: HashMap<String, String>,
}

/// æ€§èƒ½è¶‹åŠ¿
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum PerformanceTrend {
    /// æ€§èƒ½æ”¹å–„
    Improving,
    /// æ€§èƒ½ç¨³å®š
    Stable,
    /// æ€§èƒ½ä¸‹é™
    Degrading,
    /// æ€§èƒ½æ³¢åŠ¨
    Fluctuating,
}

/// æ€§èƒ½é¢„æµ‹ç»“æœ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformancePrediction {
    /// é¢„æµ‹æ—¶é—´æˆ³
    pub prediction_timestamp: u64,
    /// é¢„æµ‹çš„å“åº”æ—¶é—´
    pub predicted_response_time_ms: f64,
    /// é¢„æµ‹çš„ååé‡
    pub predicted_throughput_rps: f64,
    /// é¢„æµ‹çš„èµ„æºä½¿ç”¨ç‡
    pub predicted_resource_usage: ResourceUsageMetrics,
    /// é¢„æµ‹ç½®ä¿¡åº¦
    pub confidence: f64,
    /// é¢„æµ‹åŸºäºçš„å†å²æ•°æ®ç‚¹æ•°é‡
    pub data_points_used: usize,
}

/// æ€§èƒ½ä¼˜åŒ–å»ºè®®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceOptimizationSuggestion {
    /// å»ºè®®ID
    pub id: String,
    /// å»ºè®®æ ‡é¢˜
    pub title: String,
    /// å»ºè®®æè¿°
    pub description: String,
    /// ä¼˜åŒ–ç±»å‹
    pub optimization_type: OptimizationStrategy,
    /// é¢„æœŸæ”¹å–„ç¨‹åº¦
    pub expected_improvement: f64,
    /// å®æ–½éš¾åº¦
    pub implementation_difficulty: DifficultyLevel,
    /// å®æ–½æ­¥éª¤
    pub implementation_steps: Vec<String>,
    /// é£é™©è¯„ä¼°
    pub risk_assessment: RiskLevel,
}

/// éš¾åº¦çº§åˆ«
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum DifficultyLevel {
    /// ç®€å•
    Easy,
    /// ä¸­ç­‰
    Medium,
    /// å›°éš¾
    Hard,
    /// ä¸“å®¶çº§
    Expert,
}

/// é£é™©çº§åˆ«
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum RiskLevel {
    /// ä½é£é™©
    Low,
    /// ä¸­ç­‰é£é™©
    Medium,
    /// é«˜é£é™©
    High,
    /// æé«˜é£é™©
    Critical,
}

/// ä¼ä¸šçº§æ€§èƒ½ç›‘æ§å™¨
pub struct EnterprisePerformanceMonitor {
    /// é…ç½®
    config: PerformanceMonitorConfig,
    /// æŒ‡æ ‡æ”¶é›†å™¨
    metrics_collector: Arc<dyn MetricsCollector>,
    /// æ€§èƒ½åˆ†æå™¨
    performance_analyzer: Arc<dyn PerformanceAnalyzer>,
    /// å†å²æ€§èƒ½æ•°æ®
    performance_history: Arc<RwLock<VecDeque<RealTimePerformanceMetrics>>>,
    /// æ€§èƒ½é¢„æµ‹ç¼“å­˜
    prediction_cache: Arc<RwLock<Option<PerformancePrediction>>>,
    /// ä¼˜åŒ–å»ºè®®ç¼“å­˜
    optimization_suggestions: Arc<RwLock<Vec<PerformanceOptimizationSuggestion>>>,
    /// ç›‘æ§ç»Ÿè®¡
    monitoring_stats: Arc<RwLock<MonitoringStatistics>>,
}

/// ç›‘æ§ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct MonitoringStatistics {
    /// ç›‘æ§è¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
    pub uptime_seconds: u64,
    /// æ”¶é›†çš„æ•°æ®ç‚¹æ€»æ•°
    pub total_data_points: u64,
    /// ç”Ÿæˆçš„é¢„æµ‹æ•°é‡
    pub predictions_generated: u64,
    /// æä¾›çš„ä¼˜åŒ–å»ºè®®æ•°é‡
    pub optimization_suggestions_provided: u64,
    /// æ£€æµ‹åˆ°çš„æ€§èƒ½é—®é¢˜æ•°é‡
    pub performance_issues_detected: u64,
    /// å¹³å‡ç›‘æ§å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
    pub avg_monitoring_latency_ms: f64,
}

impl EnterprisePerformanceMonitor {
    /// åˆ›å»ºæ–°çš„ä¼ä¸šçº§æ€§èƒ½ç›‘æ§å™¨
    pub fn new(
        config: PerformanceMonitorConfig,
        metrics_collector: Arc<dyn MetricsCollector>,
        performance_analyzer: Arc<dyn PerformanceAnalyzer>,
    ) -> Self {
        Self {
            config,
            metrics_collector,
            performance_analyzer,
            performance_history: Arc::new(RwLock::new(VecDeque::new())),
            prediction_cache: Arc::new(RwLock::new(None)),
            optimization_suggestions: Arc::new(RwLock::new(Vec::new())),
            monitoring_stats: Arc::new(RwLock::new(MonitoringStatistics::default())),
        }
    }

    /// å¯åŠ¨æ€§èƒ½ç›‘æ§
    pub async fn start(&self) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        println!("ğŸš€ å¯åŠ¨ä¼ä¸šçº§æ€§èƒ½ç›‘æ§ç³»ç»Ÿ");

        // å¯åŠ¨å®æ—¶ç›‘æ§å¾ªç¯
        let monitor = self.clone();
        tokio::spawn(async move {
            monitor.monitoring_loop().await;
        });

        // å¯åŠ¨é¢„æµ‹åˆ†æå¾ªç¯
        if self.config.prediction_config.enabled {
            let monitor = self.clone();
            tokio::spawn(async move {
                monitor.prediction_loop().await;
            });
        }

        // å¯åŠ¨ä¼˜åŒ–å»ºè®®å¾ªç¯
        if self.config.auto_optimization_config.enabled {
            let monitor = self.clone();
            tokio::spawn(async move {
                monitor.optimization_loop().await;
            });
        }

        // å¯åŠ¨æ•°æ®æ¸…ç†å¾ªç¯
        let monitor = self.clone();
        tokio::spawn(async move {
            monitor.cleanup_loop().await;
        });

        Ok(())
    }

    /// å®æ—¶ç›‘æ§å¾ªç¯
    async fn monitoring_loop(&self) {
        let mut interval = tokio::time::interval(
            Duration::from_secs(self.config.monitoring_interval_seconds)
        );

        loop {
            interval.tick().await;

            let start_time = SystemTime::now();

            if let Err(e) = self.collect_performance_metrics().await {
                eprintln!("æ€§èƒ½æŒ‡æ ‡æ”¶é›†å¤±è´¥: {}", e);
            }

            // æ›´æ–°ç›‘æ§å»¶è¿Ÿç»Ÿè®¡
            let monitoring_latency = start_time.elapsed().unwrap_or_default().as_millis() as f64;
            self.update_monitoring_latency(monitoring_latency).await;
        }
    }

    /// æ”¶é›†æ€§èƒ½æŒ‡æ ‡
    async fn collect_performance_metrics(&self) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        let now = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_millis() as u64;

        // è·å–åŸºç¡€æŒ‡æ ‡
        let metrics_summary = self.metrics_collector.get_metrics_summary(None, None, None).await?;

        // æ„å»ºå®æ—¶æ€§èƒ½æŒ‡æ ‡
        let performance_metrics = RealTimePerformanceMetrics {
            timestamp: now,
            response_time: self.calculate_response_time_metrics(&metrics_summary).await,
            throughput: self.calculate_throughput_metrics(&metrics_summary).await,
            resource_usage: self.calculate_resource_usage_metrics().await,
            error_metrics: self.calculate_error_metrics(&metrics_summary).await,
            performance_trend: self.analyze_performance_trend().await,
        };

        // å­˜å‚¨åˆ°å†å²æ•°æ®
        {
            let mut history = self.performance_history.write().await;
            history.push_back(performance_metrics.clone());

            // é™åˆ¶å†å²æ•°æ®å¤§å°
            let max_data_points = (self.config.data_retention_hours * 3600) / self.config.monitoring_interval_seconds;
            while history.len() > max_data_points as usize {
                history.pop_front();
            }
        }

        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.update_monitoring_statistics().await;

        // æ£€æŸ¥æ€§èƒ½é˜ˆå€¼
        self.check_performance_thresholds(&performance_metrics).await;

        Ok(())
    }

    /// è®¡ç®—å“åº”æ—¶é—´æŒ‡æ ‡
    async fn calculate_response_time_metrics(&self, summary: &MetricsSummary) -> ResponseTimeMetrics {
        // ç®€åŒ–å®ç° - åœ¨å®é™…åº”ç”¨ä¸­åº”è¯¥ä»è¯¦ç»†çš„æ‰§è¡Œæ—¶é—´æ•°æ®è®¡ç®—
        let avg_ms = summary.avg_execution_time_ms;

        ResponseTimeMetrics {
            avg_ms,
            median_ms: avg_ms * 0.9,
            p95_ms: avg_ms * 1.5,
            p99_ms: avg_ms * 2.0,
            max_ms: avg_ms * 3.0,
            min_ms: avg_ms * 0.3,
        }
    }

    /// è®¡ç®—ååé‡æŒ‡æ ‡
    async fn calculate_throughput_metrics(&self, summary: &MetricsSummary) -> ThroughputMetrics {
        let total_requests = summary.total_executions;
        let successful_requests = summary.successful_executions;
        let failed_requests = summary.failed_executions;

        // ç®€åŒ–çš„ååé‡è®¡ç®—
        let requests_per_second = total_requests as f64 / 60.0; // å‡è®¾åŸºäº1åˆ†é’Ÿçª—å£

        ThroughputMetrics {
            requests_per_second,
            requests_per_minute: requests_per_second * 60.0,
            total_requests,
            successful_requests,
            failed_requests,
        }
    }

    /// è®¡ç®—èµ„æºä½¿ç”¨æŒ‡æ ‡
    async fn calculate_resource_usage_metrics(&self) -> ResourceUsageMetrics {
        // æ¨¡æ‹Ÿèµ„æºä½¿ç”¨æ•°æ® - åœ¨å®é™…åº”ç”¨ä¸­åº”è¯¥ä»ç³»ç»ŸAPIè·å–
        ResourceUsageMetrics {
            cpu_usage_percent: 45.0 + (rand::random::<f64>() * 20.0),
            memory_usage_percent: 60.0 + (rand::random::<f64>() * 15.0),
            disk_usage_percent: 30.0 + (rand::random::<f64>() * 10.0),
            network_bandwidth_mbps: 10.0 + (rand::random::<f64>() * 5.0),
            active_connections: 50 + (rand::random::<u64>() % 20),
        }
    }

    /// è®¡ç®—é”™è¯¯æŒ‡æ ‡
    async fn calculate_error_metrics(&self, summary: &MetricsSummary) -> ErrorMetrics {
        let error_rate_percent = if summary.total_executions > 0 {
            (summary.failed_executions as f64 / summary.total_executions as f64) * 100.0
        } else {
            0.0
        };

        let mut errors_by_type = HashMap::new();
        errors_by_type.insert("timeout".to_string(), summary.failed_executions / 3);
        errors_by_type.insert("connection".to_string(), summary.failed_executions / 4);
        errors_by_type.insert("validation".to_string(), summary.failed_executions / 5);

        let mut errors_by_severity = HashMap::new();
        errors_by_severity.insert("warning".to_string(), summary.failed_executions / 2);
        errors_by_severity.insert("error".to_string(), summary.failed_executions / 3);
        errors_by_severity.insert("critical".to_string(), summary.failed_executions / 6);

        ErrorMetrics {
            error_rate_percent,
            errors_by_type,
            errors_by_severity,
            recent_errors: Vec::new(), // ç®€åŒ–å®ç°
        }
    }

    /// åˆ†ææ€§èƒ½è¶‹åŠ¿
    async fn analyze_performance_trend(&self) -> PerformanceTrend {
        let history = self.performance_history.read().await;

        if history.len() < 3 {
            return PerformanceTrend::Stable;
        }

        // è·å–æœ€è¿‘çš„æ€§èƒ½æ•°æ®ç‚¹
        let recent_points: Vec<_> = history.iter().rev().take(5).collect();

        // è®¡ç®—å“åº”æ—¶é—´è¶‹åŠ¿
        let response_times: Vec<f64> = recent_points.iter()
            .map(|p| p.response_time.avg_ms)
            .collect();

        // ç®€å•çš„è¶‹åŠ¿åˆ†æ
        let trend_score = self.calculate_trend_score(&response_times);

        match trend_score {
            score if score > 0.1 => PerformanceTrend::Degrading,
            score if score < -0.1 => PerformanceTrend::Improving,
            score if score.abs() > 0.05 => PerformanceTrend::Fluctuating,
            _ => PerformanceTrend::Stable,
        }
    }

    /// è®¡ç®—è¶‹åŠ¿åˆ†æ•°
    fn calculate_trend_score(&self, values: &[f64]) -> f64 {
        if values.len() < 2 {
            return 0.0;
        }

        let mut trend_sum = 0.0;
        for i in 1..values.len() {
            let change = (values[i] - values[i-1]) / values[i-1];
            trend_sum += change;
        }

        trend_sum / (values.len() - 1) as f64
    }

    /// æ£€æŸ¥æ€§èƒ½é˜ˆå€¼
    async fn check_performance_thresholds(&self, metrics: &RealTimePerformanceMetrics) {
        let thresholds = &self.config.thresholds;

        if metrics.response_time.avg_ms > thresholds.response_time_ms {
            println!("âš ï¸  å“åº”æ—¶é—´è¶…è¿‡é˜ˆå€¼: {:.2}ms > {:.2}ms",
                metrics.response_time.avg_ms, thresholds.response_time_ms);
            self.increment_performance_issues().await;
        }

        if metrics.resource_usage.cpu_usage_percent > thresholds.cpu_usage_percent {
            println!("âš ï¸  CPUä½¿ç”¨ç‡è¶…è¿‡é˜ˆå€¼: {:.1}% > {:.1}%",
                metrics.resource_usage.cpu_usage_percent, thresholds.cpu_usage_percent);
            self.increment_performance_issues().await;
        }

        if metrics.error_metrics.error_rate_percent > thresholds.error_rate_percent {
            println!("âš ï¸  é”™è¯¯ç‡è¶…è¿‡é˜ˆå€¼: {:.1}% > {:.1}%",
                metrics.error_metrics.error_rate_percent, thresholds.error_rate_percent);
            self.increment_performance_issues().await;
        }
    }

    /// é¢„æµ‹åˆ†æå¾ªç¯
    async fn prediction_loop(&self) {
        let mut interval = tokio::time::interval(
            Duration::from_secs(self.config.prediction_config.prediction_window_hours * 3600 / 4)
        );

        loop {
            interval.tick().await;

            if let Err(e) = self.generate_performance_prediction().await {
                eprintln!("æ€§èƒ½é¢„æµ‹ç”Ÿæˆå¤±è´¥: {}", e);
            }
        }
    }

    /// ç”Ÿæˆæ€§èƒ½é¢„æµ‹
    async fn generate_performance_prediction(&self) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        let history = self.performance_history.read().await;

        if history.len() < 10 {
            return Ok(()); // æ•°æ®ä¸è¶³ï¼Œæ— æ³•é¢„æµ‹
        }

        // è·å–å†å²æ•°æ®ç”¨äºé¢„æµ‹
        let history_window = self.config.prediction_config.history_window_hours * 3600 / self.config.monitoring_interval_seconds;
        let recent_data: Vec<_> = history.iter()
            .rev()
            .take(history_window as usize)
            .collect();

        // ç®€åŒ–çš„çº¿æ€§é¢„æµ‹æ¨¡å‹
        let prediction = self.create_linear_prediction(&recent_data).await;

        // ç¼“å­˜é¢„æµ‹ç»“æœ
        {
            let mut cache = self.prediction_cache.write().await;
            *cache = Some(prediction);
        }

        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.increment_predictions_generated().await;

        println!("ğŸ”® ç”Ÿæˆæ€§èƒ½é¢„æµ‹ (åŸºäº{}ä¸ªæ•°æ®ç‚¹)", recent_data.len());

        Ok(())
    }

    /// åˆ›å»ºçº¿æ€§é¢„æµ‹
    async fn create_linear_prediction(&self, data: &[&RealTimePerformanceMetrics]) -> PerformancePrediction {
        let now = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_millis() as u64;

        // ç®€åŒ–çš„é¢„æµ‹ç®—æ³•
        let response_times: Vec<f64> = data.iter().map(|d| d.response_time.avg_ms).collect();
        let throughputs: Vec<f64> = data.iter().map(|d| d.throughput.requests_per_second).collect();

        let predicted_response_time = self.predict_value(&response_times);
        let predicted_throughput = self.predict_value(&throughputs);

        // é¢„æµ‹èµ„æºä½¿ç”¨
        let cpu_values: Vec<f64> = data.iter().map(|d| d.resource_usage.cpu_usage_percent).collect();
        let memory_values: Vec<f64> = data.iter().map(|d| d.resource_usage.memory_usage_percent).collect();

        let predicted_resource_usage = ResourceUsageMetrics {
            cpu_usage_percent: self.predict_value(&cpu_values),
            memory_usage_percent: self.predict_value(&memory_values),
            disk_usage_percent: 35.0, // ç®€åŒ–
            network_bandwidth_mbps: 12.0, // ç®€åŒ–
            active_connections: 55, // ç®€åŒ–
        };

        PerformancePrediction {
            prediction_timestamp: now + (self.config.prediction_config.prediction_window_hours * 3600 * 1000),
            predicted_response_time_ms: predicted_response_time,
            predicted_throughput_rps: predicted_throughput,
            predicted_resource_usage,
            confidence: 0.75, // ç®€åŒ–çš„ç½®ä¿¡åº¦
            data_points_used: data.len(),
        }
    }

    /// é¢„æµ‹æ•°å€¼
    fn predict_value(&self, values: &[f64]) -> f64 {
        if values.is_empty() {
            return 0.0;
        }

        // ç®€å•çš„ç§»åŠ¨å¹³å‡é¢„æµ‹
        let recent_avg = values.iter().rev().take(5).sum::<f64>() / 5.0_f64.min(values.len() as f64);
        let overall_avg = values.iter().sum::<f64>() / values.len() as f64;

        // åŠ æƒå¹³å‡ï¼š70%æœ€è¿‘è¶‹åŠ¿ + 30%æ•´ä½“è¶‹åŠ¿
        recent_avg * 0.7 + overall_avg * 0.3
    }

    /// ä¼˜åŒ–å»ºè®®å¾ªç¯
    async fn optimization_loop(&self) {
        let mut interval = tokio::time::interval(
            Duration::from_secs(self.config.auto_optimization_config.execution_interval_minutes * 60)
        );

        loop {
            interval.tick().await;

            if let Err(e) = self.generate_optimization_suggestions().await {
                eprintln!("ä¼˜åŒ–å»ºè®®ç”Ÿæˆå¤±è´¥: {}", e);
            }
        }
    }

    /// ç”Ÿæˆä¼˜åŒ–å»ºè®®
    async fn generate_optimization_suggestions(&self) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        let history = self.performance_history.read().await;

        if history.is_empty() {
            return Ok(());
        }

        let latest_metrics = history.back().unwrap();
        let mut suggestions = Vec::new();

        // åŸºäºå½“å‰æ€§èƒ½æŒ‡æ ‡ç”Ÿæˆå»ºè®®
        if latest_metrics.response_time.avg_ms > self.config.thresholds.response_time_ms {
            suggestions.push(self.create_response_time_optimization_suggestion(latest_metrics).await);
        }

        if latest_metrics.resource_usage.cpu_usage_percent > self.config.thresholds.cpu_usage_percent {
            suggestions.push(self.create_cpu_optimization_suggestion(latest_metrics).await);
        }

        if latest_metrics.resource_usage.memory_usage_percent > self.config.thresholds.memory_usage_percent {
            suggestions.push(self.create_memory_optimization_suggestion(latest_metrics).await);
        }

        if latest_metrics.error_metrics.error_rate_percent > self.config.thresholds.error_rate_percent {
            suggestions.push(self.create_error_rate_optimization_suggestion(latest_metrics).await);
        }

        // æ›´æ–°ä¼˜åŒ–å»ºè®®ç¼“å­˜
        {
            let mut cache = self.optimization_suggestions.write().await;
            *cache = suggestions;
        }

        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.increment_optimization_suggestions().await;

        println!("ğŸ’¡ ç”Ÿæˆäº†{}æ¡ä¼˜åŒ–å»ºè®®", self.optimization_suggestions.read().await.len());

        Ok(())
    }

    /// åˆ›å»ºå“åº”æ—¶é—´ä¼˜åŒ–å»ºè®®
    async fn create_response_time_optimization_suggestion(&self, metrics: &RealTimePerformanceMetrics) -> PerformanceOptimizationSuggestion {
        PerformanceOptimizationSuggestion {
            id: uuid::Uuid::new_v4().to_string(),
            title: "ä¼˜åŒ–å“åº”æ—¶é—´".to_string(),
            description: format!("å½“å‰å¹³å‡å“åº”æ—¶é—´ä¸º{:.2}msï¼Œè¶…è¿‡é˜ˆå€¼{:.2}ms",
                metrics.response_time.avg_ms, self.config.thresholds.response_time_ms),
            optimization_type: OptimizationStrategy::CacheOptimization,
            expected_improvement: 25.0,
            implementation_difficulty: DifficultyLevel::Medium,
            implementation_steps: vec![
                "åˆ†ææ…¢æŸ¥è¯¢å’Œçƒ­ç‚¹æ•°æ®".to_string(),
                "å®æ–½Redisç¼“å­˜å±‚".to_string(),
                "ä¼˜åŒ–æ•°æ®åº“ç´¢å¼•".to_string(),
                "å¯ç”¨HTTPç¼“å­˜å¤´".to_string(),
            ],
            risk_assessment: RiskLevel::Low,
        }
    }

    /// åˆ›å»ºCPUä¼˜åŒ–å»ºè®®
    async fn create_cpu_optimization_suggestion(&self, metrics: &RealTimePerformanceMetrics) -> PerformanceOptimizationSuggestion {
        PerformanceOptimizationSuggestion {
            id: uuid::Uuid::new_v4().to_string(),
            title: "ä¼˜åŒ–CPUä½¿ç”¨ç‡".to_string(),
            description: format!("å½“å‰CPUä½¿ç”¨ç‡ä¸º{:.1}%ï¼Œè¶…è¿‡é˜ˆå€¼{:.1}%",
                metrics.resource_usage.cpu_usage_percent, self.config.thresholds.cpu_usage_percent),
            optimization_type: OptimizationStrategy::AutoScaling,
            expected_improvement: 30.0,
            implementation_difficulty: DifficultyLevel::Easy,
            implementation_steps: vec![
                "å¯ç”¨æ°´å¹³è‡ªåŠ¨æ‰©å®¹".to_string(),
                "ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦".to_string(),
                "å®æ–½å¼‚æ­¥å¤„ç†".to_string(),
                "ä½¿ç”¨è¿æ¥æ± ".to_string(),
            ],
            risk_assessment: RiskLevel::Medium,
        }
    }

    /// åˆ›å»ºå†…å­˜ä¼˜åŒ–å»ºè®®
    async fn create_memory_optimization_suggestion(&self, metrics: &RealTimePerformanceMetrics) -> PerformanceOptimizationSuggestion {
        PerformanceOptimizationSuggestion {
            id: uuid::Uuid::new_v4().to_string(),
            title: "ä¼˜åŒ–å†…å­˜ä½¿ç”¨".to_string(),
            description: format!("å½“å‰å†…å­˜ä½¿ç”¨ç‡ä¸º{:.1}%ï¼Œè¶…è¿‡é˜ˆå€¼{:.1}%",
                metrics.resource_usage.memory_usage_percent, self.config.thresholds.memory_usage_percent),
            optimization_type: OptimizationStrategy::GarbageCollectionTuning,
            expected_improvement: 20.0,
            implementation_difficulty: DifficultyLevel::Hard,
            implementation_steps: vec![
                "åˆ†æå†…å­˜æ³„æ¼".to_string(),
                "ä¼˜åŒ–æ•°æ®ç»“æ„".to_string(),
                "è°ƒæ•´åƒåœ¾å›æ”¶å‚æ•°".to_string(),
                "å®æ–½å¯¹è±¡æ± ".to_string(),
            ],
            risk_assessment: RiskLevel::High,
        }
    }

    /// åˆ›å»ºé”™è¯¯ç‡ä¼˜åŒ–å»ºè®®
    async fn create_error_rate_optimization_suggestion(&self, metrics: &RealTimePerformanceMetrics) -> PerformanceOptimizationSuggestion {
        PerformanceOptimizationSuggestion {
            id: uuid::Uuid::new_v4().to_string(),
            title: "é™ä½é”™è¯¯ç‡".to_string(),
            description: format!("å½“å‰é”™è¯¯ç‡ä¸º{:.1}%ï¼Œè¶…è¿‡é˜ˆå€¼{:.1}%",
                metrics.error_metrics.error_rate_percent, self.config.thresholds.error_rate_percent),
            optimization_type: OptimizationStrategy::LoadBalancingAdjustment,
            expected_improvement: 40.0,
            implementation_difficulty: DifficultyLevel::Medium,
            implementation_steps: vec![
                "åˆ†æé”™è¯¯æ—¥å¿—æ¨¡å¼".to_string(),
                "å®æ–½é‡è¯•æœºåˆ¶".to_string(),
                "ä¼˜åŒ–è´Ÿè½½å‡è¡¡ç­–ç•¥".to_string(),
                "å¢å¼ºé”™è¯¯å¤„ç†".to_string(),
            ],
            risk_assessment: RiskLevel::Low,
        }
    }

    /// æ•°æ®æ¸…ç†å¾ªç¯
    async fn cleanup_loop(&self) {
        let mut interval = tokio::time::interval(Duration::from_secs(3600)); // æ¯å°æ—¶æ¸…ç†ä¸€æ¬¡

        loop {
            interval.tick().await;

            if let Err(e) = self.cleanup_old_data().await {
                eprintln!("æ•°æ®æ¸…ç†å¤±è´¥: {}", e);
            }
        }
    }

    /// æ¸…ç†è¿‡æœŸæ•°æ®
    async fn cleanup_old_data(&self) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        let mut history = self.performance_history.write().await;
        let max_data_points = (self.config.data_retention_hours * 3600) / self.config.monitoring_interval_seconds;

        while history.len() > max_data_points as usize {
            history.pop_front();
        }

        println!("ğŸ§¹ æ•°æ®æ¸…ç†å®Œæˆï¼Œä¿ç•™{}ä¸ªæ•°æ®ç‚¹", history.len());
        Ok(())
    }

    /// æ›´æ–°ç›‘æ§ç»Ÿè®¡ä¿¡æ¯
    async fn update_monitoring_statistics(&self) {
        let mut stats = self.monitoring_stats.write().await;
        stats.total_data_points += 1;
        stats.uptime_seconds += self.config.monitoring_interval_seconds;
    }

    /// æ›´æ–°ç›‘æ§å»¶è¿Ÿ
    async fn update_monitoring_latency(&self, latency_ms: f64) {
        let mut stats = self.monitoring_stats.write().await;
        let total_points = stats.total_data_points as f64;
        if total_points > 0.0 {
            stats.avg_monitoring_latency_ms =
                (stats.avg_monitoring_latency_ms * (total_points - 1.0) + latency_ms) / total_points;
        } else {
            stats.avg_monitoring_latency_ms = latency_ms;
        }
    }

    /// å¢åŠ æ€§èƒ½é—®é¢˜è®¡æ•°
    async fn increment_performance_issues(&self) {
        let mut stats = self.monitoring_stats.write().await;
        stats.performance_issues_detected += 1;
    }

    /// å¢åŠ é¢„æµ‹ç”Ÿæˆè®¡æ•°
    async fn increment_predictions_generated(&self) {
        let mut stats = self.monitoring_stats.write().await;
        stats.predictions_generated += 1;
    }

    /// å¢åŠ ä¼˜åŒ–å»ºè®®è®¡æ•°
    async fn increment_optimization_suggestions(&self) {
        let mut stats = self.monitoring_stats.write().await;
        stats.optimization_suggestions_provided += 1;
    }

    /// è·å–å½“å‰æ€§èƒ½æŒ‡æ ‡
    pub async fn get_current_performance_metrics(&self) -> Option<RealTimePerformanceMetrics> {
        let history = self.performance_history.read().await;
        history.back().cloned()
    }

    /// è·å–æ€§èƒ½å†å²æ•°æ®
    pub async fn get_performance_history(&self, limit: Option<usize>) -> Vec<RealTimePerformanceMetrics> {
        let history = self.performance_history.read().await;
        match limit {
            Some(n) => history.iter().rev().take(n).cloned().collect(),
            None => history.iter().cloned().collect(),
        }
    }

    /// è·å–æ€§èƒ½é¢„æµ‹
    pub async fn get_performance_prediction(&self) -> Option<PerformancePrediction> {
        self.prediction_cache.read().await.clone()
    }

    /// è·å–ä¼˜åŒ–å»ºè®®
    pub async fn get_optimization_suggestions(&self) -> Vec<PerformanceOptimizationSuggestion> {
        self.optimization_suggestions.read().await.clone()
    }

    /// è·å–ç›‘æ§ç»Ÿè®¡ä¿¡æ¯
    pub async fn get_monitoring_statistics(&self) -> MonitoringStatistics {
        self.monitoring_stats.read().await.clone()
    }

    /// è·å–æ€§èƒ½æ‘˜è¦æŠ¥å‘Š
    pub async fn get_performance_summary(&self) -> PerformanceSummaryReport {
        let current_metrics = self.get_current_performance_metrics().await;
        let prediction = self.get_performance_prediction().await;
        let suggestions = self.get_optimization_suggestions().await;
        let stats = self.get_monitoring_statistics().await;

        PerformanceSummaryReport {
            timestamp: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap()
                .as_millis() as u64,
            current_metrics,
            prediction,
            optimization_suggestions: suggestions,
            monitoring_statistics: stats,
            health_score: self.calculate_health_score().await,
        }
    }

    /// è®¡ç®—å¥åº·åˆ†æ•°
    async fn calculate_health_score(&self) -> f64 {
        let current_metrics = match self.get_current_performance_metrics().await {
            Some(metrics) => metrics,
            None => return 50.0, // é»˜è®¤ä¸­ç­‰å¥åº·åˆ†æ•°
        };

        let mut score = 100.0;
        let thresholds = &self.config.thresholds;

        // å“åº”æ—¶é—´è¯„åˆ†
        if current_metrics.response_time.avg_ms > thresholds.response_time_ms {
            let penalty = (current_metrics.response_time.avg_ms / thresholds.response_time_ms - 1.0) * 20.0;
            score -= penalty.min(25.0);
        }

        // CPUä½¿ç”¨ç‡è¯„åˆ†
        if current_metrics.resource_usage.cpu_usage_percent > thresholds.cpu_usage_percent {
            let penalty = (current_metrics.resource_usage.cpu_usage_percent / thresholds.cpu_usage_percent - 1.0) * 15.0;
            score -= penalty.min(20.0);
        }

        // å†…å­˜ä½¿ç”¨ç‡è¯„åˆ†
        if current_metrics.resource_usage.memory_usage_percent > thresholds.memory_usage_percent {
            let penalty = (current_metrics.resource_usage.memory_usage_percent / thresholds.memory_usage_percent - 1.0) * 15.0;
            score -= penalty.min(20.0);
        }

        // é”™è¯¯ç‡è¯„åˆ†
        if current_metrics.error_metrics.error_rate_percent > thresholds.error_rate_percent {
            let penalty = (current_metrics.error_metrics.error_rate_percent / thresholds.error_rate_percent - 1.0) * 25.0;
            score -= penalty.min(30.0);
        }

        score.max(0.0).min(100.0)
    }
}

/// æ€§èƒ½æ‘˜è¦æŠ¥å‘Š
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceSummaryReport {
    /// æŠ¥å‘Šæ—¶é—´æˆ³
    pub timestamp: u64,
    /// å½“å‰æ€§èƒ½æŒ‡æ ‡
    pub current_metrics: Option<RealTimePerformanceMetrics>,
    /// æ€§èƒ½é¢„æµ‹
    pub prediction: Option<PerformancePrediction>,
    /// ä¼˜åŒ–å»ºè®®
    pub optimization_suggestions: Vec<PerformanceOptimizationSuggestion>,
    /// ç›‘æ§ç»Ÿè®¡ä¿¡æ¯
    pub monitoring_statistics: MonitoringStatistics,
    /// å¥åº·åˆ†æ•° (0-100)
    pub health_score: f64,
}

// ä¸ºEnterprisePerformanceMonitorå®ç°Clone trait
impl Clone for EnterprisePerformanceMonitor {
    fn clone(&self) -> Self {
        Self {
            config: self.config.clone(),
            metrics_collector: self.metrics_collector.clone(),
            performance_analyzer: self.performance_analyzer.clone(),
            performance_history: self.performance_history.clone(),
            prediction_cache: self.prediction_cache.clone(),
            optimization_suggestions: self.optimization_suggestions.clone(),
            monitoring_stats: self.monitoring_stats.clone(),
        }
    }
}

impl Default for PerformanceMonitorConfig {
    fn default() -> Self {
        Self {
            monitoring_interval_seconds: 10,
            data_retention_hours: 24,
            thresholds: PerformanceThresholds {
                response_time_ms: 1000.0,
                cpu_usage_percent: 80.0,
                memory_usage_percent: 85.0,
                error_rate_percent: 5.0,
                throughput_rps: 100.0,
            },
            prediction_config: PredictionConfig {
                enabled: true,
                prediction_window_hours: 1,
                history_window_hours: 6,
                accuracy_threshold: 0.8,
            },
            auto_optimization_config: AutoOptimizationConfig {
                enabled: false, // é»˜è®¤å…³é—­è‡ªåŠ¨ä¼˜åŒ–
                strategies: vec![
                    OptimizationStrategy::AutoScaling,
                    OptimizationStrategy::CacheOptimization,
                ],
                execution_interval_minutes: 30,
            },
        }
    }
}
