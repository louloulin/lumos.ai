/*!
# API Server Module

ç‹¬ç«‹çš„APIæœåŠ¡å™¨ï¼Œæä¾›AIèŠå¤©å’Œç®¡ç†åŠŸèƒ½ã€‚

## åŠŸèƒ½ç‰¹æ€§

- **RESTful API**: æ ‡å‡†çš„HTTP APIæ¥å£
- **æµå¼å“åº”**: Server-Sent Eventsæ”¯æŒ
- **CORSæ”¯æŒ**: è·¨åŸŸèµ„æºå…±äº«
- **é”™è¯¯å¤„ç†**: ç»Ÿä¸€çš„é”™è¯¯å“åº”æ ¼å¼
*/

use axum::{
    extract::{Path, State},
    http::{header, Method, StatusCode},
    response::{IntoResponse, Json},
    routing::{delete, get, post},
    Router,
};
use serde_json::json;
use std::net::SocketAddr;
use tower::ServiceBuilder;
use tower_http::cors::{Any, CorsLayer};

use crate::ai_client::{AIClient, AIClientConfig, AIProvider};
use crate::streaming::{self, AppState};

/// å¯åŠ¨APIæœåŠ¡å™¨
pub async fn start_api_server() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸš€ Starting LumosAI API Server...");

    // åˆ›å»ºAIå®¢æˆ·ç«¯
    let ai_client = create_ai_client();
    let app_state = AppState { ai_client };

    // é…ç½®CORS
    let cors = CorsLayer::new()
        .allow_origin(Any)
        .allow_methods([Method::GET, Method::POST, Method::PUT, Method::DELETE])
        .allow_headers([header::CONTENT_TYPE, header::AUTHORIZATION]);

    // æ„å»ºè·¯ç”±
    let app = Router::new()
        // å¥åº·æ£€æŸ¥
        .route("/health", get(streaming::health_check))
        
        // èŠå¤©API
        .route("/api/chat/stream", post(streaming::stream_chat))
        .route("/api/chat/simple", post(streaming::simple_chat))
        
        // å¯¹è¯ç®¡ç†
        .route("/api/conversations", get(list_conversations))
        .route("/api/conversations/:id", get(streaming::get_conversation))
        .route("/api/conversations/:id", delete(streaming::delete_conversation))
        
        // AIæ¨¡å‹ç®¡ç†
        .route("/api/models", get(list_models))
        .route("/api/models/:id", get(get_model))
        
        // é…ç½®ç®¡ç†
        .route("/api/config", get(get_config))
        .route("/api/config", post(update_config))
        
        // é™æ€æ–‡ä»¶å’Œæ–‡æ¡£
        .route("/", get(api_info))
        .route("/docs", get(api_docs))
        
        .layer(ServiceBuilder::new().layer(cors))
        .with_state(app_state);

    // å¯åŠ¨æœåŠ¡å™¨
    let addr = SocketAddr::from(([127, 0, 0, 1], 3001));
    println!("ğŸ“¡ API Server listening on http://{}", addr);
    println!("ğŸ“š API Documentation: http://{}/docs", addr);
    
    let listener = tokio::net::TcpListener::bind(addr).await?;
    axum::serve(listener, app).await?;

    Ok(())
}

/// åˆ›å»ºAIå®¢æˆ·ç«¯
fn create_ai_client() -> AIClient {
    // ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    if let Ok(openai_key) = std::env::var("OPENAI_API_KEY") {
        println!("ğŸ”‘ Using OpenAI API");
        return AIClient::openai(openai_key);
    }
    
    if let Ok(deepseek_key) = std::env::var("DEEPSEEK_API_KEY") {
        println!("ğŸ”‘ Using DeepSeek API");
        return AIClient::deepseek(deepseek_key);
    }
    
    // é»˜è®¤ä½¿ç”¨æœ¬åœ°Ollama
    println!("ğŸ  Using local Ollama");
    AIClient::ollama(
        "http://localhost:11434/v1".to_string(),
        "llama2".to_string(),
    )
}

/// APIä¿¡æ¯
async fn api_info() -> impl IntoResponse {
    Json(json!({
        "name": "LumosAI API",
        "version": env!("CARGO_PKG_VERSION"),
        "description": "AIèŠå¤©å’Œç®¡ç†APIæœåŠ¡",
        "endpoints": {
            "health": "/health",
            "chat_stream": "/api/chat/stream",
            "chat_simple": "/api/chat/simple",
            "conversations": "/api/conversations",
            "models": "/api/models",
            "config": "/api/config",
            "docs": "/docs"
        }
    }))
}

/// APIæ–‡æ¡£
async fn api_docs() -> impl IntoResponse {
    let docs = r#"
# LumosAI API Documentation

## èŠå¤©API

### æµå¼èŠå¤©
```
POST /api/chat/stream
Content-Type: application/json

{
    "message": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
    "conversation_id": "optional_conversation_id",
    "model": "optional_model_name"
}
```

### ç®€å•èŠå¤©
```
POST /api/chat/simple
Content-Type: application/json

{
    "message": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
    "conversation_id": "optional_conversation_id",
    "model": "optional_model_name"
}
```

## å¯¹è¯ç®¡ç†

### è·å–å¯¹è¯åˆ—è¡¨
```
GET /api/conversations
```

### è·å–ç‰¹å®šå¯¹è¯
```
GET /api/conversations/{id}
```

### åˆ é™¤å¯¹è¯
```
DELETE /api/conversations/{id}
```

## æ¨¡å‹ç®¡ç†

### è·å–å¯ç”¨æ¨¡å‹
```
GET /api/models
```

### è·å–æ¨¡å‹è¯¦æƒ…
```
GET /api/models/{id}
```

## é…ç½®ç®¡ç†

### è·å–é…ç½®
```
GET /api/config
```

### æ›´æ–°é…ç½®
```
POST /api/config
Content-Type: application/json

{
    "provider": "openai",
    "api_key": "your_api_key",
    "model": "gpt-3.5-turbo",
    "temperature": 0.7,
    "max_tokens": 2048
}
```
"#;

    (
        StatusCode::OK,
        [("content-type", "text/markdown")],
        docs
    )
}

/// è·å–å¯¹è¯åˆ—è¡¨
async fn list_conversations() -> impl IntoResponse {
    // TODO: ä»æ•°æ®åº“è·å–å¯¹è¯åˆ—è¡¨
    Json(json!({
        "conversations": [],
        "total": 0,
        "page": 1,
        "per_page": 20
    }))
}

/// è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
async fn list_models() -> impl IntoResponse {
    Json(json!({
        "models": [
            {
                "id": "gpt-3.5-turbo",
                "name": "GPT-3.5 Turbo",
                "provider": "openai",
                "description": "å¿«é€Ÿã€ç»æµçš„é€šç”¨æ¨¡å‹"
            },
            {
                "id": "gpt-4",
                "name": "GPT-4",
                "provider": "openai",
                "description": "æœ€å¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹"
            },
            {
                "id": "deepseek-chat",
                "name": "DeepSeek Chat",
                "provider": "deepseek",
                "description": "é«˜æ€§èƒ½çš„ä¸­æ–‡å¯¹è¯æ¨¡å‹"
            },
            {
                "id": "llama2",
                "name": "Llama 2",
                "provider": "ollama",
                "description": "å¼€æºçš„æœ¬åœ°è¿è¡Œæ¨¡å‹"
            }
        ]
    }))
}

/// è·å–ç‰¹å®šæ¨¡å‹ä¿¡æ¯
async fn get_model(Path(model_id): Path<String>) -> impl IntoResponse {
    // TODO: ä»é…ç½®æˆ–æ•°æ®åº“è·å–æ¨¡å‹ä¿¡æ¯
    Json(json!({
        "id": model_id,
        "name": format!("Model {}", model_id),
        "provider": "unknown",
        "description": "Model description",
        "capabilities": ["chat", "completion"],
        "context_length": 4096,
        "max_tokens": 2048
    }))
}

/// è·å–å½“å‰é…ç½®
async fn get_config() -> impl IntoResponse {
    Json(json!({
        "provider": "openai",
        "model": "gpt-3.5-turbo",
        "temperature": 0.7,
        "max_tokens": 2048,
        "stream": true,
        "available_providers": ["openai", "deepseek", "anthropic", "ollama"]
    }))
}

/// æ›´æ–°é…ç½®
async fn update_config(Json(config): Json<serde_json::Value>) -> impl IntoResponse {
    // TODO: éªŒè¯å’Œä¿å­˜é…ç½®
    Json(json!({
        "success": true,
        "message": "é…ç½®å·²æ›´æ–°",
        "config": config
    }))
}
